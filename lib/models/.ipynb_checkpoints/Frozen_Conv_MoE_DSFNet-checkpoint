import torch
from torch import nn
import torch.nn.functional as F
import os

# å¯¼å…¥åŸå§‹çš„DSFNetä½œä¸ºæˆ‘ä»¬çš„â€œä¸“å®¶â€
from .DSFNet import DSFNet as DSFNet_expert
from .stNet import load_model  # æˆ‘ä»¬å°†ä½¿ç”¨æ‚¨é¡¹ç›®ä¸­çš„load_modelå‡½æ•°


class SparseGatingNetwork(nn.Module):
    """
    ç¨€ç–é—¨æ§ç½‘ç»œï¼šä¸ºæ¯ä¸ªä¸“å®¶ç”Ÿæˆä¸€ä¸ªåˆ†æ•°ï¼Œå¹¶é€‰æ‹©åˆ†æ•°æœ€é«˜çš„Top-Kä¸ªä¸“å®¶ã€‚
    è¿™ä¸ªç½‘ç»œæ˜¯å”¯ä¸€éœ€è¦è®­ç»ƒçš„éƒ¨åˆ†ã€‚
    """

    def __init__(self, num_experts, top_k=1):
        super(SparseGatingNetwork, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k

        # ä¸€ä¸ªè½»é‡çº§çš„CNNæ¥æå–å…¨å±€ç‰¹å¾
        self.feature_extractor = nn.Sequential(
            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=1),
            nn.BatchNorm3d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),

            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(inplace=True),
        )

        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Linear(32, self.num_experts)

    def forward(self, x):
        b = x.shape[0]
        features = self.feature_extractor(x)
        features = self.avg_pool(features)
        features = features.view(b, -1)
        logits = self.fc(features)

        top_k_gates, top_k_indices = torch.topk(logits, self.top_k, dim=1)

        sparse_gates = torch.zeros_like(logits)
        # ä½¿ç”¨ softmax å¯¹ top_k çš„æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        sparse_gates.scatter_(1, top_k_indices, F.softmax(top_k_gates, dim=1))

        # è¾…åŠ©æŸå¤± (Load Balancing Loss)
        expert_mask = torch.zeros_like(logits)
        expert_mask.scatter_(1, top_k_indices, 1)
        density = expert_mask.mean(dim=0)
        density_proxy = F.softmax(logits, dim=1).mean(dim=0)
        aux_loss = (density * density_proxy).sum() * self.num_experts

        return sparse_gates, top_k_indices, aux_loss


class FrozenExpert_MoE_DSFNet(nn.Module):
    """
    ä¸€ä¸ªæ‹¥æœ‰å†»ç»“ä¸“å®¶çš„MoEæ¨¡å‹ï¼Œåªè®­ç»ƒé—¨æ§ç½‘ç»œã€‚
    """

    def __init__(self, heads, head_conv=128, num_experts=3, top_k=1, loss_coef=1e-2, pretrained_paths=None):
        super(FrozenExpert_MoE_DSFNet, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.loss_coef = loss_coef
        self.heads = heads

        print("ğŸš€ åˆå§‹åŒ– Frozen-Expert-MoE-DSFNet æ¨¡å‹")
        print(f"   - ä¸“å®¶æ€»æ•°: {self.num_experts}")
        print(f"   - æ¿€æ´»ä¸“å®¶æ•° (Top-K): {self.top_k}")

        # 1. å®ä¾‹åŒ–é—¨æ§ç½‘ç»œ
        self.gating_network = SparseGatingNetwork(self.num_experts, self.top_k)

        # 2. å®ä¾‹åŒ–ã€åŠ è½½å¹¶å†»ç»“ä¸“å®¶
        self.experts = nn.ModuleList()
        if pretrained_paths is None or len(pretrained_paths) != self.num_experts:
            raise ValueError(f"å¿…é¡»æä¾›ä¸€ä¸ªåŒ…å« {self.num_experts} ä¸ªé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„çš„åˆ—è¡¨ã€‚")

        for i, path in enumerate(pretrained_paths):
            print(f"   - åŠ è½½å¹¶å†»ç»“ä¸“å®¶ {i + 1}/{self.num_experts} ä»: '{os.path.basename(path)}'")
            expert_model = DSFNet_expert(heads, head_conv)

            # ä½¿ç”¨æ‚¨é¡¹ç›®ä¸­çš„ load_model å‡½æ•°åŠ è½½æƒé‡
            if os.path.exists(path):
                expert_model = load_model(expert_model, path)
            else:
                print(f"   - âš ï¸ è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨ {path}ã€‚ä¸“å®¶ {i + 1} å°†ä½¿ç”¨éšæœºæƒé‡ï¼")

            # ==================== [æ ¸å¿ƒæ­¥éª¤: å†»ç»“ä¸“å®¶] ====================
            expert_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            for param in expert_model.parameters():
                param.requires_grad = False
            # ==========================================================

            self.experts.append(expert_model)

        print("âœ… æ‰€æœ‰ä¸“å®¶å·²åŠ è½½å¹¶å†»ç»“ã€‚")

    def forward(self, x):
        batch_size = x.shape[0]

        # 1. è·å–é—¨æ§è¾“å‡º
        sparse_gates, top_k_indices, aux_loss = self.gating_network(x)

        # 2. åˆå§‹åŒ–æœ€ç»ˆè¾“å‡º
        final_outputs = {head: 0.0 for head in self.heads}

        # 3. éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
        for i in range(batch_size):
            active_indices = top_k_indices[i]
            active_gates = sparse_gates[i][active_indices]
            sample_input = x[i].unsqueeze(0)

            # 4. éå†è¢«é€‰ä¸­çš„ä¸“å®¶
            for k, expert_idx in enumerate(active_indices):
                # åªæœ‰è¢«é€‰ä¸­çš„ä¸“å®¶ä¼šæ‰§è¡Œå‰å‘ä¼ æ’­
                with torch.no_grad():  # å†æ¬¡ç¡®è®¤ä¸“å®¶ä¸è®¡ç®—æ¢¯åº¦
                    expert_output_dict = self.experts[expert_idx](sample_input)[0]

                gate_val = active_gates[k]

                # 5. åŠ æƒç´¯åŠ ç»“æœ
                for head_name, head_tensor in expert_output_dict.items():
                    # ç¡®ä¿ head_tensor æ˜¯ [1, C, H, W]
                    if i == 0 and k == 0:
                        # é¦–æ¬¡å¾ªç¯æ—¶ï¼Œåˆå§‹åŒ– final_outputs å¼ é‡
                        if not isinstance(final_outputs[head_name], torch.Tensor):
                            final_outputs[head_name] = torch.zeros(batch_size, *head_tensor.shape[1:], device=x.device)

                    final_outputs[head_name][i] += gate_val * head_tensor.squeeze(0)

        return [final_outputs], self.loss_coef * aux_loss